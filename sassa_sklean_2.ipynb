{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sassa-sklean-2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNEW8SZQlfh8BwxOl458CV/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chocobearz/SASSA-Sklean/blob/main/sassa_sklean_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_X6RxRpRFyBE"
      },
      "source": [
        "# In this workshop we will use sklean to do some classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgvpka1DF3xj"
      },
      "source": [
        "Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zo_EgwdHE049"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Calling the dataset\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIqjbGnkF5Wc"
      },
      "source": [
        "Check out the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytW6hOyaFCIO"
      },
      "source": [
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDIVIvfkF7qC"
      },
      "source": [
        "See what classes we have"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wh0bH-uEFDSr"
      },
      "source": [
        "dataset['species'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0vgMnxRE-Jj"
      },
      "source": [
        "Split the dataset into test, train and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQH9tbHoFKrL"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a Training & Test set (80% training, 20% testing)\n",
        "train, test = train_test_split(dataset, test_size=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT8Bi14uFLWj"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZnWOINmGQ-P"
      },
      "source": [
        "Split to validation set (could also use cross validation, but we will use a validation set)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qavcE8afFNdc"
      },
      "source": [
        "# Set aside validation set (validation set is 25% of the training data)\n",
        "# Overall, leaves us with 60% training, 20% validation, 20% testing\n",
        "train, validate = train_test_split(train, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBXExzjhGhKe"
      },
      "source": [
        "Start the decision tree. [Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZD4IdUeFRm1"
      },
      "source": [
        "# Decision Tree Classifier\n",
        "# ************************\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the classifier\n",
        "classifier = DecisionTreeClassifier(max_depth=5) ## THIS LINE\n",
        "\n",
        "# Separate the Features from the Target\n",
        "train_X = train.drop('species', axis=1) # features\n",
        "train_y = train['species'] # target\n",
        "\n",
        "# Train the classifier\n",
        "classifier.fit(train_X, train_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFP9bTOsG2Ir"
      },
      "source": [
        "Plot the tree.\n",
        "Boo matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCvp-MV7FUgX"
      },
      "source": [
        "from sklearn.tree import plot_tree\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=[10, 10])\n",
        "\n",
        "plot_tree(classifier, \n",
        "          feature_names=train.columns, \n",
        "          class_names=classifier.classes_)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85MKDIWNG_hI"
      },
      "source": [
        "Test on validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyWl7nsvFXG6"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "predictions = classifier.predict(train_X)\n",
        "print('----- Training -----')\n",
        "print('Accuracy:', accuracy_score(train_y, predictions))\n",
        "print('Precision:', list(zip(classifier.classes_, precision_score(train_y, predictions, average=None))))\n",
        "print('Recall:', list(zip(classifier.classes_, recall_score(train_y, predictions, average=None))))\n",
        "\n",
        "# Separate the Features from the Target\n",
        "validate_X = validate.drop('species', axis=1)\n",
        "validate_y = validate['species']\n",
        "\n",
        "predictions = classifier.predict(validate_X)\n",
        "print('\\n----- Validation -----')\n",
        "print('Accuracy:', accuracy_score(validate_y, predictions))\n",
        "print('Precision:', list(zip(classifier.classes_, precision_score(validate_y, predictions, average=None))))\n",
        "print('Recall:', list(zip(classifier.classes_, recall_score(validate_y, predictions, average=None))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agIrg8veFZpZ"
      },
      "source": [
        "precision_score(train_y, predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7BcdM4LHINO"
      },
      "source": [
        "Some options for tuning:\n",
        "\n",
        "`max_depth`: The maximum depth of the learned tree. Reducing this value shortens the graph and limits how many rules the tree can learn.\n",
        "\n",
        "`min_samples_leaf`: The minimum samples allowed at a leaf node. Increaing this value prevents splits to be made the seperate a small number of nodes (think about seperating on ID number).\n",
        "\n",
        "`class_weight`: how much weight examples from different classes should be given in finding the optimal splits. This can help deal with class-imbalance, a common problem in data science."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMJPlW7GHYi1"
      },
      "source": [
        "Evaluate the model with test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OycIyQYFdRc"
      },
      "source": [
        "# Separate the Testing features from the Target\n",
        "test_X = test.drop('species', axis=1)\n",
        "test_y = test['species']\n",
        "\n",
        "predictions = classifier.predict(test_X)\n",
        "print('\\n----- Testing -----')\n",
        "print('Accuracy:', accuracy_score(test_y, predictions))\n",
        "print('Precision:', list(zip(classifier.classes_, precision_score(test_y, predictions, average=None))))\n",
        "print('Recall:', list(zip(classifier.classes_, recall_score(test_y, predictions, average=None))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6zgEtBLHfsq"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PttvRPkkuS0Q"
      },
      "source": [
        "\n",
        "###**Accuracy**\n",
        "Is the proportion of correctly classified examples. \n",
        "\n",
        "Accuracy is perhaps the most common metric for evaluating classification problems.However, there are a few problems with it. \n",
        "\n",
        "First, **it doesn't account for differences in importance between classes.** \n",
        "\n",
        "Second, **accuracy doesn't deal with the class imbalance**\n",
        "\n",
        "###**Precision**\n",
        "For binary classification problems (where we can think of classes as Positive and Negative), precision is the percentage of predicted positives which are truly positive.\n",
        "\n",
        "For multi-class classification problems (when we have more than 2 classes), precision is calculated for each class individually. We can choose to calculate the average precision over all classes, or leave it as a class-by-class measure. \n",
        "\n",
        "###**Recall**\n",
        "For binary classification problems, recall is the percentage of truly positive examples which were predicted as positive.\n",
        "\n",
        "As with precision, recall is calculated on a class-by-class basis for multi-class classification problems.\n",
        "\n",
        "###**F Measure**\n",
        "Precision and recall are often at odds with one another (very high recall often requires taking a hit to performance). The F-measure (or f1-score) is an attempt to balance precision and recall into a single metric.\n",
        "\n",
        "F-Measure gives equal importance to precision and recall while in reality one is often more important than the other (determined by the problem itself).\n",
        "\n",
        "As with precision and recall, F-measure is calculated per class for classification with more than 2 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eULMkffxIdWD"
      },
      "source": [
        "##Logistic Regression example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1h2zXoaLFh10"
      },
      "source": [
        "# Import algorithm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# select features & target\n",
        "X = dataset.drop('species', axis=1)\n",
        "y = dataset['species']\n",
        "\n",
        "# split the dataset\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, \n",
        "                                                            y, \n",
        "                                                            test_size=0.2, \n",
        "                                                            random_state=0)\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, \n",
        "                                                  y_train_val, \n",
        "                                                  test_size=0.25, \n",
        "                                                  random_state=0)\n",
        "\n",
        "# Train the classifier\n",
        "log_classifier = LogisticRegression()\n",
        "log_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Validate & Adjust\n",
        "predictions = log_classifier.predict(X_val)\n",
        "print('\\n----- Validation -----')\n",
        "print('Accuracy:', accuracy_score(y_val, predictions))\n",
        "print('Precision:', list(zip(log_classifier.classes_, precision_score(y_val, predictions, average=None))))\n",
        "print('Recall:', list(zip(log_classifier.classes_, recall_score(y_val, predictions, average=None))))\n",
        "\n",
        "# ! Hyper-parameter tuning !\n",
        "print(log_classifier)\n",
        "# Uncomment to evaluate on the test set\n",
        "predictions = log_classifier.predict(X_test)\n",
        "print('\\n----- Testing -----')\n",
        "print('Accuracy:', accuracy_score(y_test, predictions))\n",
        "print('Precision:', list(zip(log_classifier.classes_, precision_score(y_test, predictions, average=None))))\n",
        "print('Recall:', list(zip(log_classifier.classes_, recall_score(y_test, predictions, average=None))))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}